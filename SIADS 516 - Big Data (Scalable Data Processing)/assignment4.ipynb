{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a74a2f6f90017bcef716d63d8dc2e470",
     "grade": false,
     "grade_id": "cell-3c17da45e7b7645f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# SIADS 516: Homework 4\n",
    "\n",
    "- **Dr. Chris Teplovs**, School of Information, University of Michigan\n",
    "- **Kris Steinhoff**, School of Information, University of Michigan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d1c62f6c256ca075d290009bc3f4883c",
     "grade": false,
     "grade_id": "cell-02367b8cfece2a65",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# The AutograderHelper class provides methods used by the autograder.\n",
    "from autograder_helper import AutograderHelper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0477ef5614295bf8caf6e1096266bfa3",
     "grade": true,
     "grade_id": "inject_private_helper",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder cell. This cell is worth 0 points.\n",
    "# This cell has hidden code used to configure the autograder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fd552a41671f29be3f94d538b25e5520",
     "grade": false,
     "grade_id": "cell-34b72f966bd3b156",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "This homework assignment uses the Yelp Academic dataset, with which you should now be familiar.\n",
    "We have created a few cells to get you started, but you're largely on your own to devise solutions to the\n",
    "\"real-world\" questions below.\n",
    "\n",
    "In this assignment, provide solutions that use spark.sql() calls to query the dataset. For example, to find the answer to \"How many users have more than 100 \"cool\" votes?\", this:\n",
    "```\n",
    "query = \"\"\"\n",
    "SELECT count(*) FROM user WHERE cool > 100\n",
    "\"\"\"\n",
    "spark.sql(query).show()\n",
    "```\n",
    "is similar to:\n",
    "```\n",
    "user.filter('cool > 100').show()\n",
    "```\n",
    "But in this assignment, use the first approach. The autograder will check for the use of `spark.sql()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c34ef5c67d3c510a686b5e18d499db08",
     "grade": false,
     "grade_id": "cell-e1638183f41aa602",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Our usual Spark mantra:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7cea91b07d9a4bee525e3ed6d23d2d37",
     "grade": false,
     "grade_id": "cell-98c1a867a2e9c83b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/07/25 15:07:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('My First Spark application') \\\n",
    "    .getOrCreate() \n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ba476caea9d89d002deede468dff6f08",
     "grade": false,
     "grade_id": "cell-c9ad81a944f5fd79",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Load the JSON files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f46ca5ab7e2f08c41b5b4014d63c248e",
     "grade": false,
     "grade_id": "cell-a04e6cbc86f81133",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/07/25 15:07:47 WARN Utils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "business = spark.read.json('../../assets/data/yelp_academic/yelp_academic_dataset_business.json.gz')\n",
    "checkin = spark.read.json('../../assets/data/yelp_academic/yelp_academic_dataset_checkin.json.gz')\n",
    "review = spark.read.json('../../assets/data/yelp_academic/yelp_academic_dataset_review.json.gz')\n",
    "tip = spark.read.json('../../assets/data/yelp_academic/yelp_academic_dataset_tip.json.gz')\n",
    "user = spark.read.json('../../assets/data/yelp_academic/yelp_academic_dataset_user.json.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e5cbdf5d60302f8ad654c74be9880857",
     "grade": false,
     "grade_id": "cell-811b05fe37080393",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Create temp views for the DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8d1689383e4a34655a2f6790f22691d2",
     "grade": false,
     "grade_id": "cell-29e2438127b26344",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "business.createOrReplaceTempView(\"business\")\n",
    "checkin.createOrReplaceTempView(\"checkin\")\n",
    "tip.createOrReplaceTempView(\"tip\")\n",
    "review.createOrReplaceTempView(\"review\")\n",
    "user.createOrReplaceTempView(\"user\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c883cedcf0fb749a8a224c2a0b882241",
     "grade": false,
     "grade_id": "cell-41d7ac10cefde0c6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "\n",
    "## -- EXAMPLE PROBLEM --\n",
    "\n",
    "Get a list of users named \"Kahlil\" with the number of their reviews tagged \"funny\".\n",
    "\n",
    "- The result should have these columns:\n",
    "  - `user_id`\n",
    "  - `name`\n",
    "  - `funny`\n",
    "- The result rows do NOT need to be ordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "80f9661399390d017037f14f4a94a307",
     "grade": false,
     "grade_id": "cell-371a339af443edea",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Solve the problem by assigning populating the provided variable \n",
    "# with the result of the Spark SQL query\n",
    "\n",
    "def users_kahlil():\n",
    "    return spark.sql(\"\"\"\\\n",
    "        SELECT user_id, name, funny\n",
    "        FROM user\n",
    "        WHERE name = \"Kahlil\"\n",
    "        \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e3441139d044cfec2158d33419aa5ef3",
     "grade": false,
     "grade_id": "cell-582768a3d45c6227",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 5:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+-----+\n",
      "|             user_id|  name|funny|\n",
      "+--------------------+------+-----+\n",
      "|HE5fZW8m7MpdLHa3H...|Kahlil|   32|\n",
      "|BAX7MdujQiv_Camqi...|Kahlil|    0|\n",
      "|fepcVUPERVRA16b4M...|Kahlil|    0|\n",
      "|uvG9MAZF6vIVBoj24...|Kahlil|    4|\n",
      "|sEQtegzBDjARGB_YM...|Kahlil|    0|\n",
      "|JpOCv0TtT2nz0gv0S...|Kahlil|    0|\n",
      "+--------------------+------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# It can be helpful to look at the result with .show()\n",
    "\n",
    "results = users_kahlil()\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1acaa97b83eedf3a03da66db98b5c041",
     "grade": false,
     "grade_id": "cell-560e6a3ee3481757",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# This notebook provides several asserts for each problem. \n",
    "#\n",
    "# There are also hidden tests that are run by the autograder after submission.\n",
    "\n",
    "assert type(users_kahlil()) == pyspark.sql.dataframe.DataFrame, \\\n",
    "    \"The return value should be a Spark DataFrame.\"\n",
    "\n",
    "AutograderHelper.assert_function_calls(users_kahlil, [\"spark.sql\"])\n",
    "\n",
    "users_kahlil_ids = [r[\"user_id\"] for r in users_kahlil().collect()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4840215fe85ae16d6280cd6937b8ffb5",
     "grade": false,
     "grade_id": "cell-490ace1873a7a27d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert len(users_kahlil_ids) == 6, \\\n",
    "    \"The result must have 6 rows.\"\n",
    "\n",
    "expected_user_id = \"HE5fZW8m7MpdLHa3HGp1FA\"\n",
    "assert expected_user_id in users_kahlil_ids, f'The user_id column should include \"{expected_user_id}\"'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6cc9f25f3d16a0319f4dde019b12e304",
     "grade": false,
     "grade_id": "cell-0676172ebb6ff73e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "\n",
    "## -- USERS WITH 500 FANS --\n",
    "\n",
    "Determine how many users have more than 500 fans.\n",
    "\n",
    "- The result should have 1 column and 1 row\n",
    "- The name of the column does not matter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5088ccde26bc2fe148a019ae565b079a",
     "grade": false,
     "grade_id": "cell-537c97258ccb754f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def count_users_500_fans():\n",
    "    # YOUR CODE HERE\n",
    "#     raise NotImplementedError()\n",
    "\n",
    "    query=\"\"\"\n",
    "          SELECT COUNT(*) FROM user WHERE fans>500    \n",
    "    \"\"\"\n",
    "    \n",
    "    return spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0f28a8bda0029e9a3b213c43655eb377",
     "grade": false,
     "grade_id": "cell-d94241d861c907fc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "assert type(count_users_500_fans()) == pyspark.sql.dataframe.DataFrame, \\\n",
    "    \"The return value should be a Spark DataFrame.\"\n",
    "\n",
    "AutograderHelper.assert_function_calls(count_users_500_fans, [\"spark.sql\"])\n",
    "\n",
    "count_users_500_fans_submitted = count_users_500_fans().collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ad53801803c512af9e8fd879f548485f",
     "grade": false,
     "grade_id": "cell-5fddef372fe914cd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert count_users_500_fans_submitted != 8286, \\\n",
    "    \"That is the number of users who have more than 500 funny ratings.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b7615729333618d8c70c02082d5ef5c2",
     "grade": true,
     "grade_id": "cell-1942c8a3a624ffb1",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder cell. This cell is worth 2 points (out of 20). This cell contains hidden tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3bbfd7152afbf1abd8cd32d42c045826",
     "grade": false,
     "grade_id": "cell-fd1250c02e03df22",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## -- BUSINESS REVIEWS --\n",
    "\n",
    "Determine how many businesses have at least 4 stars and at least 100 reviews.\n",
    "\n",
    "- The result should have 1 column and 1 row\n",
    "- The name of the column does not matter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6445fb6ebc9bd4e1b9d88ed933ad89da",
     "grade": false,
     "grade_id": "cell-aa6b3ab08f12e6bc",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def business_reviews_count():\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "#     raise NotImplementedError()\n",
    "\n",
    "    query=\"\"\"\n",
    "          SELECT COUNT(*) FROM business WHERE stars>=4 AND review_count>=100\n",
    "    \"\"\"\n",
    "    \n",
    "    return spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "93f88f37b60238580c7b3f40540f1c1f",
     "grade": false,
     "grade_id": "cell-517deb36fd365dbb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "assert type(business_reviews_count()) == pyspark.sql.dataframe.DataFrame, \\\n",
    "    \"The return value should be a Spark DataFrame.\"\n",
    "\n",
    "AutograderHelper.assert_function_calls(business_reviews_count, [\"spark.sql\"])\n",
    "\n",
    "business_reviews_count_submitted = business_reviews_count().collect()[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1cae6796829b85cfc2f51efd603c3eb9",
     "grade": false,
     "grade_id": "cell-7717dbfafa0d39a7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert business_reviews_count_submitted != 2814, \\\n",
    "    (\n",
    "        \"2814 is the number of businesses with greater than 4 stars (you should include ones with 4 stars) \"\n",
    "        \"and greater than 100 reviews (you should include ones with 100 reviews).\"\n",
    "    )\n",
    "\n",
    "assert business_reviews_count_submitted != 7397, \\\n",
    "    (\n",
    "        \"7397 is the number of businesses with at least 4 stars and greater than 100 reviews (you should \"\n",
    "        \"include ones with 100 reviews).\"\n",
    "    )\n",
    "assert business_reviews_count_submitted != 2842, \\\n",
    "    (\n",
    "        \"2842 is the number of businesses with greater than 4 stars (you should include ones with 4 stars) \"\n",
    "        \"and at least 100 reviews.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a1381e2d87a0493f1aa1d5fe9b72c895",
     "grade": true,
     "grade_id": "cell-51af07a99d707639",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder cell. This cell is worth 2 points (out of 20). This cell contains hidden tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8a7ef6f188593d4b5030e7ed6e49f226",
     "grade": false,
     "grade_id": "cell-42472798054d0d9b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## -- LITCHFIELD OHIO --\n",
    "\n",
    "Get a list of businesses from Litchfield, OH. \n",
    "\n",
    "- The result should have these columns:\n",
    "  - `business_id`\n",
    "  - `name`\n",
    "- The result rows do NOT need to be ordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b36262572746a2bd83cbc0c878447bdd",
     "grade": false,
     "grade_id": "cell-7363eb8cf90fcf97",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def litchfield_oh_businesses():\n",
    "    # YOUR CODE HERE\n",
    "#     raise NotImplementedError()\n",
    "\n",
    "    query=\"\"\"\n",
    "          SELECT business_id, name FROM business WHERE city='Litchfield' AND state='OH'          \n",
    "    \"\"\"\n",
    "    \n",
    "    return spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "453501462dacf5af43da8b0925f77294",
     "grade": false,
     "grade_id": "cell-a51f95eccd981b88",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "AutograderHelper.assert_function_calls(litchfield_oh_businesses, [\"spark.sql\"])\n",
    "\n",
    "assert type(litchfield_oh_businesses()) == pyspark.sql.dataframe.DataFrame, \\\n",
    "    \"The return value should be a Spark DataFrame.\"\n",
    "\n",
    "litchfield_oh_business_names = [r[\"name\"] for r in litchfield_oh_businesses().collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "05d283ba5ada33fbdaba6371a2682ee4",
     "grade": false,
     "grade_id": "cell-1338e0791d051601",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert \"Tonios Pizza\" in litchfield_oh_business_names, \"'Tonios Pizza' should appear in the result.\"\n",
    "assert \"Hayseed\" not in litchfield_oh_business_names, \"'Hayseed' should not appear in the result.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "67db5e100119f2a5cde4552acd6b9bc0",
     "grade": true,
     "grade_id": "cell-1a157c9a6f7f7a1f",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder cell. This cell is worth 2 points (out of 20). This cell contains hidden tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0ef7d39ff1c920ab801e0ee1dff33be7",
     "grade": false,
     "grade_id": "cell-3f3489b42178d651",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## -- US STATES --\n",
    "\n",
    "Determine which US states are represented in the data set. (The file `../../assets/data/states.csv` contains a list of US state names and abbreviations.)\n",
    "\n",
    "- The result should have this columns:\n",
    "  - `state` (the full name of the state in the dataset)\n",
    "- The result rows do NOT need to be ordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ba12ffb90c7bab834fda2bf63d9a04c2",
     "grade": false,
     "grade_id": "cell-f330007b1e3ba3d6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def states_names_in_data():\n",
    "    # YOUR CODE HERE\n",
    "#     raise NotImplementedError()\n",
    "\n",
    "    states=spark.read.csv(\"../../assets/data/states.csv\", header=True)\n",
    "    states.createOrReplaceTempView(\"states\")\n",
    "    \n",
    "    query=\"\"\"\n",
    "          SELECT DISTINCT states.state FROM states INNER JOIN business ON states.abbreviation=business.state\n",
    "    \"\"\"\n",
    "    \n",
    "    return spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9b369380da8eba07d7e5fc96b801d2e7",
     "grade": false,
     "grade_id": "cell-d5065f190b219da0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "AutograderHelper.assert_function_calls(states_names_in_data, [\"spark.sql\"])\n",
    "\n",
    "assert type(states_names_in_data()) == pyspark.sql.dataframe.DataFrame, \\\n",
    "    \"The return value should be a Spark DataFrame.\"\n",
    "\n",
    "state_names_list = [r[\"state\"] for r in states_names_in_data().collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1712707f8d8625d5b2fe744deb770fcb",
     "grade": false,
     "grade_id": "cell-bd73363da3322d4c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert \"North Carolina\" in state_names_list, \"North Carolina should appear in the result.\"\n",
    "assert \"Michigan\" not in state_names_list, \"Michigan should appear in the result.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9839b561d0c9ba9fad99b30d988c4424",
     "grade": true,
     "grade_id": "cell-3010e95bc0521807",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder cell. This cell is worth 3 points (out of 20). This cell contains hidden tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1d3217f0d9d2d8a6d9a4b2951c3f41f7",
     "grade": false,
     "grade_id": "cell-7753d29b0c281dfa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## -- FUNNIEST REVIEW --\n",
    "\n",
    "Determine the text of the funniest review.\n",
    "\n",
    "- The result should have 1 column and 1 row\n",
    "- The name of the column does not matter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7e2f098f481d3ae1835bdd1768826240",
     "grade": false,
     "grade_id": "cell-95b0fdabbf98d3eb",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def funniest_review():\n",
    "    # YOUR CODE HERE\n",
    "#     raise NotImplementedError()\n",
    "    \n",
    "    query=\"\"\"\n",
    "          SELECT text FROM review WHERE funny=\n",
    "          (\n",
    "              SELECT MAX(funny) FROM review\n",
    "          )\n",
    "    \"\"\"\n",
    "    \n",
    "    return spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "45a2b86718ca945d64a9238295a0bcc6",
     "grade": false,
     "grade_id": "cell-5083e8c7dede5798",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "AutograderHelper.assert_function_calls(funniest_review, [\"spark.sql\"])\n",
    "\n",
    "assert type(funniest_review()) == pyspark.sql.dataframe.DataFrame, \\\n",
    "    \"The return value should be a Spark DataFrame.\"\n",
    "\n",
    "funniest_review_first_row = funniest_review().take(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "62ac4502ea7300718d8cc474a2d91b31",
     "grade": false,
     "grade_id": "cell-2baca7013c81c4af",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "funniest_review_len = len(funniest_review_first_row[0])\n",
    "assert funniest_review_len == 421, \\\n",
    "    f\"Hint: the funniest review has 421 characters (found {funniest_review_len})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2ec3033d5a959ca235fdfb5dae30417f",
     "grade": true,
     "grade_id": "cell-584ad14e0596f259",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder cell. This cell is worth 2 points (out of 20). This cell contains hidden tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "29765caa33002d5576230cf9889b39ef",
     "grade": false,
     "grade_id": "cell-7cc4c307effb2a93",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## -- REVIEW WORD COUNT -- \n",
    "\n",
    "Find the 10 reviews with the largest word counts.\n",
    "\n",
    "- Use a UDF to determine review word counts using the Python `.split()` method on the string object.\n",
    "- The result should have this columns:\n",
    "  - `word_count`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "babffd8d58c0bb3fb0f2b6336d087e94",
     "grade": false,
     "grade_id": "cell-8441242abddc3e6e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def reviews_top_10_word_counts():\n",
    "    # YOUR CODE HERE\n",
    "#     raise NotImplementedError()\n",
    "\n",
    "    from pyspark.sql.functions import udf\n",
    "    from pyspark.sql.types import IntegerType\n",
    "    \n",
    "    def word_counts(string):\n",
    "        return len(string.split())\n",
    "    \n",
    "    word_counts_int=udf(lambda x: word_counts(x), IntegerType())\n",
    "    \n",
    "    spark.udf.register(\"word_counts_int\", word_counts_int)\n",
    "    \n",
    "    query=\"\"\"\n",
    "          SELECT word_counts_int(text) AS word_count FROM review \n",
    "          ORDER BY word_count DESC \n",
    "          LIMIT 10\n",
    "    \"\"\"\n",
    "    \n",
    "    return spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "35d3f6c3a83ecc1cc92462941f904ae3",
     "grade": false,
     "grade_id": "cell-6c9c5cb31aa50db9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "AutograderHelper.assert_function_calls(reviews_top_10_word_counts, [\"spark.sql\"])\n",
    "\n",
    "assert type(reviews_top_10_word_counts()) == pyspark.sql.dataframe.DataFrame, \\\n",
    "    \"The return value should be a Spark DataFrame.\"\n",
    "\n",
    "reviews_top_10_word_counts_first_row = reviews_top_10_word_counts().take(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f3382fedf9df6110a54b26caa4fc8ef4",
     "grade": false,
     "grade_id": "cell-3926048e177b2f94",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert reviews_top_10_word_counts_first_row[\"word_count\"] == 1056, \"The first word_count should be 1056\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fd16055a949d37754dd300549680c48b",
     "grade": true,
     "grade_id": "cell-83b2d963d716eb3d",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder cell. This cell is worth 3 points (out of 20). This cell contains hidden tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a27f76f7880166a9c941da0e18c5b325",
     "grade": false,
     "grade_id": "cell-bd7bf1d844733fc3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## -- MOST TIPS --\n",
    "\n",
    "Determine the names of the top 100 users who provided the most tips.\n",
    "\n",
    "- The result should have these columns:\n",
    "  - `name`\n",
    "  - `tip_count`\n",
    "- The result should be sorted by highest-to-lowest tip_count, in the case of tip_count ties, the results should be sorted by name alphabetically. For example (this is fake data):\n",
    "  ```\n",
    "  +--------+---------+\n",
    "  |    name|tip_count|\n",
    "  +--------+---------+\n",
    "  | Weifong|      167|\n",
    "  |   Alice|       42|\n",
    "  |     Bob|       42|\n",
    "  |   Jamal|        3|\n",
    "  +--------+---------+\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "93ea286599ba9517d563657bdd15b8da",
     "grade": false,
     "grade_id": "cell-2f35829e7a961ef8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def users_top_100_tip_count():\n",
    "    # YOUR CODE HERE\n",
    "#     raise NotImplementedError()\n",
    "\n",
    "    query=\"\"\"\n",
    "          SELECT name, tip_count FROM\n",
    "          (\n",
    "              SELECT user.name as name, COUNT(tip.user_id) AS tip_count \n",
    "              FROM user INNER JOIN tip\n",
    "              ON user.user_id=tip.user_id\n",
    "              GROUP BY name, tip.user_id\n",
    "              ORDER BY tip_count DESC, name\n",
    "          )\n",
    "          LIMIT 100\n",
    "        \"\"\"\n",
    "\n",
    "    return spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9ef7d66ddd244609657f7beff1e63c8c",
     "grade": false,
     "grade_id": "cell-6b04bf655b76bca2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "AutograderHelper.assert_function_calls(users_top_100_tip_count, [\"spark.sql\"])\n",
    "\n",
    "assert type(users_top_100_tip_count()) == pyspark.sql.dataframe.DataFrame, \\\n",
    "    \"The return value should be a Spark DataFrame.\"\n",
    "\n",
    "users_top_100_tip_count_first_row = users_top_100_tip_count().take(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "da59d9355490fc1b90bad1dc3655d5c5",
     "grade": false,
     "grade_id": "cell-36b874e02c6055dc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert users_top_100_tip_count_first_row[\"name\"] == \"Momo\", \"The first name should be Momo\"\n",
    "assert users_top_100_tip_count_first_row[\"tip_count\"] == 2439, \"The first tip_count should be 2439\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "40ca4b9036d9a9d0d5eff308cd099c74",
     "grade": true,
     "grade_id": "cell-346b2d93ee722e0e",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder cell. This cell is worth 3 points (out of 20). This cell contains hidden tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f77cb744e5efb635b2148423d45c37c7",
     "grade": false,
     "grade_id": "cell-3ea13d4f07b462c8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## -- ARIZONA SUMMARY -- \n",
    "\n",
    "List the names, number of reviews of businesses in Arizona ('AZ') and total number of reviews of the top 10 users (as determined by who has created the most number of reviews of businesses in Arizona). Include a column that shows the percentage of reviews that are of businesses from Arizona. \n",
    "\n",
    "- The result should have these columns:\n",
    "  - `name`\n",
    "  - `az_count`\n",
    "  - `total_count`\n",
    "  - `percent` (this will only be checked to within 0.01)\n",
    "- The result should be sorted by highest-to-lowest `az_count`, in the case of `az_count ties`, the results should be sorted by highest-to-lowest `percent`\n",
    "\n",
    "\n",
    "\n",
    "The first row of the results should be:\n",
    "```\n",
    "+--------+--------+-----------+---------+\n",
    "|    name|az_count|total_count|  percent|\n",
    "+--------+--------+-----------+---------+\n",
    "|    Brad|    1637|       1642|99.695496|\n",
    "+--------+--------+-----------+---------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['address',\n",
       " 'attributes',\n",
       " 'business_id',\n",
       " 'categories',\n",
       " 'city',\n",
       " 'hours',\n",
       " 'is_open',\n",
       " 'latitude',\n",
       " 'longitude',\n",
       " 'name',\n",
       " 'postal_code',\n",
       " 'review_count',\n",
       " 'stars',\n",
       " 'state']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "business.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(business_id='ujmEBvifdJM6h6RLv4wQIg', cool=0, date='2013-05-07 04:34:36', funny=1, review_id='Q1sbwvVQXV2734tPgoKj4Q', stars=1.0, text='Total bill for this horrible service? Over $8Gs. These crooks actually had the nerve to charge us $69 for 3 pills. I checked online the pills can be had for 19 cents EACH! Avoid Hospital ERs at all costs.', useful=6, user_id='hG7b0MtEbXx5QzbzE6C_VA')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['business_id',\n",
       " 'cool',\n",
       " 'date',\n",
       " 'funny',\n",
       " 'review_id',\n",
       " 'stars',\n",
       " 'text',\n",
       " 'useful',\n",
       " 'user_id']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(average_stars=4.03, compliment_cool=1, compliment_cute=0, compliment_funny=1, compliment_hot=2, compliment_list=0, compliment_more=0, compliment_note=1, compliment_photos=0, compliment_plain=1, compliment_profile=0, compliment_writer=2, cool=25, elite='2015,2016,2017', fans=5, friends='c78V-rj8NQcQjOI8KP3UEA, alRMgPcngYSCJ5naFRBz5g, ajcnq75Z5xxkvUSmmJ1bCg, BSMAmp2-wMzCkhTfq9ToNg, jka10dk9ygX76hJG0gfPZQ, dut0e4xvme7QSlesOycHQA, l4l5lBnK356zBua7B-UJ6Q, 0HicMOOs-M_gl2eO-zES4Q, _uI57wL2fLyftrcSFpfSGQ, T4_Qd0YWbC3co6WSMw4vxg, iBRoLWPtWmsI1kdbE9ORSA, xjrUcid6Ymq0DoTJELkYyw, GqadWVzJ6At-vgLzK_SKgA, DvB13VJBmSnbFXBVBsKmDA, vRP9nQkYTeNioDjtxZlVhg, gT0A1iN3eeQ8EMAjJhwQtw, 6yCWjFPtp_AD4x93WAwmnw, 1dKzpNnib-JlViKv8_Gt5g, 3Bv4_JxHXq-gVLOxYMQX0Q, ikQyfu1iViYh8T0us7wiFQ, f1GGltNaB7K5DR1jf3dOmg, tgeFUChlh7v8bZFVl2-hjQ, -9-9oyXlqsMG2he5xIWdLQ, Adj9fBPVJad8vSs-mIP7gw, Ce49RY8CKXVsTifxRYFTsw, M1_7TLi8CbdA89nFLlH4iw, wFsNv-hqbW_F5-IRqfBN6g, 0Q1L7zXHocaUZ2gsG2XJeg, cBFgmOCBdhYa0xoFEAzp_g, VrD_AgiFvzqtlR15vir3SQ, cpE-7HK514Sr5vpSen9CEQ, F1UYelhPFB-zIKlt0ygIZg, CQAL1hvsLMCzuJf9AglsXw, 1KnY1wr15WfEWIRLB9IS6g, QWFQ-kXBiLbid-lm5Jr3dQ, nymT8liFugCrM16lTy0ZfQ, qj69bdd885heDvUPCyHd2Q, DySCZZcgbdrlHgEovk5y9w, lZMJIDuvhT9Dy4KyquLXyA, b_9Gn7wS93AoPZPR0dIJqQ, N07g1IaLh0_6sUjtiSRe4w, YdfPX_7DxSnKvvdCJ57iOw, 8GYryZPD22W7WgQ8kvMkEQ, cpQmAgOWatghp14h1pn1dQ, EnchhymLYMqftCRjqvVWmw, -JdfKhFktE7Zs9BMDFcPeQ, uWhC9eof98zPkvsalgaqJw, eyTlNDDaiPatfe6mheIZ0g, VfHq0o73aKsODvfAhwAQtg, kvD5tICngLAaQDujSFWupA, dXacwEhqi9-3_XT6JeH0Og, NfU0zDaTMEQ4-X9dbQWd9A, cTHWBdjSKbctSUIvWsgFxw, 3IEtCbSDF5t7RkZ20T6s9A, HJJXTrp6UybYyPdQ9DA0JA, JaXogQFVjzGRAeBvzamBHg, NUonfKkjS1iVqnNITtgXZg, D5vaJAYp0sOrGfsj9qvsMA, H27Ecbwwu4FGAlLgICourw, S8HrLmMiE4u8FWYWkNEoTw, Io36Y3xWQcIX9rYvPcYfXQ, J5mcqh8KxYpqjaLBNlwcig, -nTB3_08g06fD0GT8AtDBQ, wMpFA46lihK8oFns_5p65A, RZGFJHeomGJCWp3xcL3ejA, ZoQSzzXoSP1RxOD4Amv9Bg, qzM0EB0SkuuGIFv0adjQAQ, HuM6vvuveken-fPZ7d4olA, H3oukHpGpn9n_mJwSDSQyQ, PkmsJsQ8FIZe8eh8c_u96g, wSByVbwME4MzgkJaFyfvNg, YEVqknoDmrHAoUbHX0nPnA, li3vsK1XAPmeJYAUTYflHQ, MKc8yXi0glbPYt0Qb4PECw, fQPH6W9fksi27gkuUPnFaA, amrCMrDsoRetYFg2kwwdFA, 84dVQ6n6r2ezNaTuc7RkKA, yW9QjWY0olv5-uRKv3t_Kw, 5XJDj7c3eoidfQ3jW18Zgw, txSc6a6pIDctvwyBeu7Aqg, HFbbDCyyqP9xPkUlcxeIdg, hTUv5oh2do6Z3OppPuuiJA, gSqonG9J4fNM-fl_fE71AA, pd9mgTFpBTg5F9x-MsczNg, j3VE22V2GcHiH8UZxfFLfw, NYXlMW-T-3V4Jqr4r-i0Wg, btxgAZedxX8IWhMifA7Xkg, -Hp5mPLiRJNFnyeX5Ygzag, P6-DwVg6-t2JuQwIUEk0iQ, OI2TvxYvZrAodBG_RF53Xw, bHxf_VPKmZur1Bier-6A2A, Et_Sb39cVm81_Xe9HDM8ZQ, 5HwGl2UyYbaRq8aD6YC-fA, ZK228WMcCKLo5thcjD7rdw, iTf8wojwfm0NOi7dOiz3Nw, btYRxQYNJjpecflNHtFH0A, Kgo42FzpW_dXFgDKoewbtg, MNk_1Q_dqOY3xxHZKeO8VQ, AlwD504T9k0m5lkg3k5g6Q', funny=17, name='Rashmi', review_count=95, useful=84, user_id='l6BmjZMeQD3rDxWUbiAiow', yelping_since='2013-10-08 23:11:33')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['average_stars',\n",
       " 'compliment_cool',\n",
       " 'compliment_cute',\n",
       " 'compliment_funny',\n",
       " 'compliment_hot',\n",
       " 'compliment_list',\n",
       " 'compliment_more',\n",
       " 'compliment_note',\n",
       " 'compliment_photos',\n",
       " 'compliment_plain',\n",
       " 'compliment_profile',\n",
       " 'compliment_writer',\n",
       " 'cool',\n",
       " 'elite',\n",
       " 'fans',\n",
       " 'friends',\n",
       " 'funny',\n",
       " 'name',\n",
       " 'review_count',\n",
       " 'useful',\n",
       " 'user_id',\n",
       " 'yelping_since']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "85877f47ca43829a5c491ca9ed0f6e75",
     "grade": false,
     "grade_id": "cell-58c41d2bcaae305b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def arizona_summary():\n",
    "    # YOUR CODE HERE\n",
    "#     raise NotImplementedError()\n",
    "\n",
    "#     query=\"\"\"\n",
    "#           SELECT name, az_count, total_count, (az_count/total_count)*100 AS percent FROM\n",
    "#           (\n",
    "#           SELECT user.name AS name, SUM(business.review_count) AS az_count, SUM(user.review_count) as total_count FROM\n",
    "#           business INNER JOIN review\n",
    "#           ON business.business_id=review.business_id\n",
    "#           INNER JOIN user\n",
    "#           ON user.user_id=review.user_id\n",
    "#           WHERE business.state='AZ'\n",
    "#           GROUP BY user.name, review.user_id\n",
    "#           )\n",
    "#           ORDER BY az_count DESC, percent DESC\n",
    "#     \"\"\"\n",
    "\n",
    "#     query=\"\"\"\n",
    "#               SELECT name, az_count, total_count, (az_count/total_count)*100 AS percent FROM\n",
    "#               (\n",
    "#               SELECT user.name AS name, SUM(business.review_count) AS az_count, SUM(user.review_count) as total_count FROM \n",
    "#               user INNER JOIN review \n",
    "#               ON user.user_id=review.user_id \n",
    "#               INNER JOIN business\n",
    "#               ON business.business_id=review.business_id\n",
    "#               WHERE business.state='AZ'\n",
    "#               )\n",
    "#               ORDER BY az_count DESC, percent DESC\n",
    "#         \"\"\"\n",
    "\n",
    "#     query=\"\"\"\n",
    "#               SELECT name, az_count, total_count, (az_count/total_count)*100 AS percent FROM\n",
    "#               (\n",
    "#               SELECT user.name AS name, az_count=\n",
    "#               (\n",
    "#               SELECT SUM(business.review_count) FROM business WHERE business.state='AZ'\n",
    "#               ), \n",
    "#               SUM(user.review_count) as total_count FROM\n",
    "#               user INNER JOIN review \n",
    "#               ON user.user_id=review.user_id \n",
    "#               INNER JOIN business\n",
    "#               ON business.business_id=review.business_id\n",
    "#               WHERE business.state='AZ'\n",
    "#               )\n",
    "#               ORDER BY az_count DESC, percent DESC\n",
    "#         \"\"\"\n",
    "\n",
    "#     query=\"\"\"\n",
    "#           SELECT name, az_count, total_count, (az_count/total_count)*100 AS percent FROM\n",
    "#           (\n",
    "#           SELECT user.name, review.user_id AS name FROM\n",
    "#           user INNER JOIN review\n",
    "#           ON user.user_id=review.user_id\n",
    "          \n",
    "#           )\n",
    "#     \"\"\"\n",
    "\n",
    "#     query=\"\"\"\n",
    "#                SELECT name, az_count, total_count, (az_count/total_count)*100 AS percent FROM\n",
    "#                (\n",
    "#                SELECT user.name AS name, SUM(business.review_count) AS az_count, SUM(user.review_count) as total_count FROM\n",
    "#                business INNER JOIN review\n",
    "#                ON business.business_id=review.business_id\n",
    "#                HAVING business.state='AZ'\n",
    "#                INNER JOIN user\n",
    "#                ON user.user_id=review.user_id\n",
    "#                GROUP BY user.name, review.user_id\n",
    "               \n",
    "#                )\"\"\"\n",
    "\n",
    "#     query=\"\"\"\n",
    "#               SELECT name, az_count, total_count, (az_count/total_count)*100 AS percent FROM\n",
    "#               (\n",
    "#               SELECT user.name AS name, SUM(business.review_count) FILTER(WHERE business.state='AZ') AS `az_count`, \n",
    "#               SUM(user.review_count) as total_count FROM\n",
    "#               business INNER JOIN review\n",
    "#               ON business.business_id=review.business_id\n",
    "#               INNER JOIN user\n",
    "#               ON user.user_id=review.user_id\n",
    "#               WHERE business.state='AZ'\n",
    "#               GROUP BY user.name, review.user_id\n",
    "#               )\n",
    "#               ORDER BY az_count DESC, percent DESC\n",
    "#         \"\"\"\n",
    "             \n",
    "    \n",
    "    spark.sql(query).show()\n",
    "    \n",
    "    return spark.sql(query)\n",
    "\n",
    "# user.name as name, COUNT(tip.user_id) AS tip_count \n",
    "#           FROM user INNER JOIN tip\n",
    "#           ON user.user_id=tip.user_id\n",
    "#           GROUP BY name, tip.user_id\n",
    "#           ORDER BY tip_count DESC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "800a4d9c3edd45935a64020e6c9b834d",
     "grade": false,
     "grade_id": "cell-9c8b7064d916064b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ParseException",
     "evalue": "\"\\nmismatched input 'FROM' expecting <EOF>(line 2, pos 88)\\n\\n== SQL ==\\n\\n              SELECT name, az_count, total_count, (az_count/total_count)*100 AS percent FROM\\n----------------------------------------------------------------------------------------^^^\\n              (\\n              SELECT user.name AS name, SUM(business.review_count), FILTER(WHERE business.state='AZ') AS `az_count`, \\n              SUM(user.review_count) as total_count FROM\\n              business INNER JOIN review\\n              ON business.business_id=review.business_id\\n              INNER JOIN user\\n              ON user.user_id=review.user_id\\n              WHERE business.state='AZ'\\n              GROUP BY user.name, review.user_id\\n              )\\n              ORDER BY az_count DESC, percent DESC\\n        \\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o24.sql.\n: org.apache.spark.sql.catalyst.parser.ParseException: \nmismatched input 'FROM' expecting <EOF>(line 2, pos 88)\n\n== SQL ==\n\n              SELECT name, az_count, total_count, (az_count/total_count)*100 AS percent FROM\n----------------------------------------------------------------------------------------^^^\n              (\n              SELECT user.name AS name, SUM(business.review_count), FILTER(WHERE business.state='AZ') AS `az_count`, \n              SUM(user.review_count) as total_count FROM\n              business INNER JOIN review\n              ON business.business_id=review.business_id\n              INNER JOIN user\n              ON user.user_id=review.user_id\n              WHERE business.state='AZ'\n              GROUP BY user.name, review.user_id\n              )\n              ORDER BY az_count DESC, percent DESC\n        \n\n\tat org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:241)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:117)\n\tat org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:48)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:69)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:643)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mParseException\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_92/1733099628.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mAutograderHelper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_function_calls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marizona_summary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"spark.sql\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marizona_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;34m\"The return value should be a Spark DataFrame.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_92/238475715.py\u001b[0m in \u001b[0;36marizona_summary\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m         \"\"\"\n\u001b[0;32m--> 767\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.parser.ParseException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mParseException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.streaming.StreamingQueryException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mStreamingQueryException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mParseException\u001b[0m: \"\\nmismatched input 'FROM' expecting <EOF>(line 2, pos 88)\\n\\n== SQL ==\\n\\n              SELECT name, az_count, total_count, (az_count/total_count)*100 AS percent FROM\\n----------------------------------------------------------------------------------------^^^\\n              (\\n              SELECT user.name AS name, SUM(business.review_count), FILTER(WHERE business.state='AZ') AS `az_count`, \\n              SUM(user.review_count) as total_count FROM\\n              business INNER JOIN review\\n              ON business.business_id=review.business_id\\n              INNER JOIN user\\n              ON user.user_id=review.user_id\\n              WHERE business.state='AZ'\\n              GROUP BY user.name, review.user_id\\n              )\\n              ORDER BY az_count DESC, percent DESC\\n        \\n\""
     ]
    }
   ],
   "source": [
    "AutograderHelper.assert_function_calls(arizona_summary, [\"spark.sql\"])\n",
    "\n",
    "assert type(arizona_summary()) == pyspark.sql.dataframe.DataFrame, \\\n",
    "    \"The return value should be a Spark DataFrame.\"\n",
    "\n",
    "arizona_summary_first_row = arizona_summary().take(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 30:===============================================>      (175 + 4) / 200]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+\n",
      "|name|total_count|\n",
      "+----+-----------+\n",
      "|Brad|         12|\n",
      "|Brad|       2944|\n",
      "|Brad|        987|\n",
      "|Brad|         22|\n",
      "|Brad|         14|\n",
      "|Brad|         54|\n",
      "|Brad|          2|\n",
      "|Brad|          4|\n",
      "|Brad|          6|\n",
      "|Brad|        192|\n",
      "+----+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# spark.sql(\"\"\"\n",
    "# SELECT name, total_count FROM\n",
    "# ( \n",
    "# SELECT user.name as name, review.user_id, SUM(user.review_count) as total_count\n",
    "# FROM user inner join review on user.user_id=review.user_id inner join business on business.business_id=review.business_id \n",
    "# group by user.name, review.user_id\n",
    "# having name='Brad'\n",
    "# limit 10\n",
    "# )\n",
    "# \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 36:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+\n",
      "|name|total_count|\n",
      "+----+-----------+\n",
      "|Brad|       1642|\n",
      "+----+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# spark.sql(\"\"\"\n",
    "# select name, review_count as total_count from user where name=\"Brad\" order by total_count desc limit 1\n",
    "# \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|         business_id|state|\n",
      "+--------------------+-----+\n",
      "|1SWheh84yJXfytovI...|   AZ|\n",
      "|xvX2CttrVhyG2z1dF...|   AZ|\n",
      "|Y6iyemLX_oylRpnr3...|   AZ|\n",
      "|1Dfx3zM-rW4n-31Ke...|   AZ|\n",
      "|giC3pVVFxCRR89rAp...|   AZ|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# spark.sql(\"\"\"\n",
    "# select business_id, state from business where state='AZ' limit 5\n",
    "# \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 53:================================================>     (178 + 4) / 200]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------+\n",
      "|             user_id|         business_id|review_count|\n",
      "+--------------------+--------------------+------------+\n",
      "|8k3aO-mPeyhbR5HUu...|J-U6C8FgveGsDdHwR...|       13278|\n",
      "|8k3aO-mPeyhbR5HUu...|ii8sAGBexBOJoYRFa...|       13278|\n",
      "|8k3aO-mPeyhbR5HUu...|DfgZlNgKwBvCpA_0a...|       13278|\n",
      "|8k3aO-mPeyhbR5HUu...|6Q7-wkCPc1KF75jZL...|       13278|\n",
      "|8k3aO-mPeyhbR5HUu...|z7GAft_JrFNmAH5xb...|       13278|\n",
      "+--------------------+--------------------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 53:=====================================================>(197 + 3) / 200]\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# spark.sql(\"\"\"\n",
    "# select review.user_id, business.business_id, user.review_count from review inner join \n",
    "# business on review.business_id=business.business_id inner join user on user.user_id=review.user_id\n",
    "# order by review_count desc\n",
    "# limit 5\n",
    "# \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ParseException",
     "evalue": "\"\\nmismatched input '(' expecting <EOF>(line 2, pos 67)\\n\\n== SQL ==\\n\\nselect user.name, review.user_id, sum(business.review_count) filter(where business.state='AZ') as az_count \\n-------------------------------------------------------------------^^^\\nfrom user inner join review \\non user.user_id=review.user_id\\ninner join business on business.businesS_id=review.business_id\\ngroup by user.name, review.user_id\\norder by az_count desc\\nlimit 10\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o24.sql.\n: org.apache.spark.sql.catalyst.parser.ParseException: \nmismatched input '(' expecting <EOF>(line 2, pos 67)\n\n== SQL ==\n\nselect user.name, review.user_id, sum(business.review_count) filter(where business.state='AZ') as az_count \n-------------------------------------------------------------------^^^\nfrom user inner join review \non user.user_id=review.user_id\ninner join business on business.businesS_id=review.business_id\ngroup by user.name, review.user_id\norder by az_count desc\nlimit 10\n\n\tat org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:241)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:117)\n\tat org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:48)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:69)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:643)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mParseException\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_92/459869042.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0morder\u001b[0m \u001b[0mby\u001b[0m \u001b[0maz_count\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mlimit\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \"\"\").show()\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m         \"\"\"\n\u001b[0;32m--> 767\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.parser.ParseException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mParseException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.streaming.StreamingQueryException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mStreamingQueryException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mParseException\u001b[0m: \"\\nmismatched input '(' expecting <EOF>(line 2, pos 67)\\n\\n== SQL ==\\n\\nselect user.name, review.user_id, sum(business.review_count) filter(where business.state='AZ') as az_count \\n-------------------------------------------------------------------^^^\\nfrom user inner join review \\non user.user_id=review.user_id\\ninner join business on business.businesS_id=review.business_id\\ngroup by user.name, review.user_id\\norder by az_count desc\\nlimit 10\\n\""
     ]
    }
   ],
   "source": [
    "# spark.sql(\"\"\"\n",
    "# select user.name, review.user_id, sum(business.review_count) as az_count \n",
    "# from user inner join review \n",
    "# on user.user_id=review.user_id\n",
    "# inner join business on business.businesS_id=review.business_id\n",
    "# group by user.name, review.user_id\n",
    "# order by az_count desc\n",
    "# limit 10\n",
    "# \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql(\"\"\"\n",
    "# SELECT name, total_count FROM\n",
    "#           (\n",
    "#           SELECT user.name as name, COUNT(review.user_id) AS total_count \n",
    "#           FROM user INNER JOIN review\n",
    "#           ON user.user_id=tip.user_id\n",
    "#           GROUP BY name, tip.user_id\n",
    "#           ORDER BY tip_count DESC\n",
    "#           )\n",
    "#           LIMIT 100\n",
    "# \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql(\"\"\"\n",
    "# SELECT user.name as name, count(review.user_id) as total_count\n",
    "# from \n",
    "# \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fedeecbede385e1397cf962a758affb4",
     "grade": false,
     "grade_id": "cell-8a0509444e910378",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert arizona_summary_first_row[\"name\"] == \"Brad\", \"The first name should be Brad\"\n",
    "assert arizona_summary_first_row[\"az_count\"] == 1637, \"The first az_count should be 1637\"\n",
    "assert arizona_summary_first_row[\"total_count\"] == 1642, \"The first total_count should be 1642\"\n",
    "\n",
    "assert round(arizona_summary_first_row[\"percent\"], 2) == 99.70, \\\n",
    "    (\n",
    "        f\"The first percent should be about 99.70 (checking to \"\n",
    "        f\"nearest 0.01, found {arizona_summary_first_row['percent']})\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "30cf9136c21d70444bff3d6085f217c1",
     "grade": true,
     "grade_id": "cell-a5e177b11e33c438",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder cell. This cell is worth 3 points (out of 20). This cell contains hidden tests."
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "mads_big_data_scalable_data_processing_v3_assignment4"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
