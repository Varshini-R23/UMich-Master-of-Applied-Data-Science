{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ff8a6b6a84b18fbf",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Homework 3: Recurrent Neural Networks (100 points)\n",
    "\n",
    "### Overview\n",
    "\n",
    "We now move from image recognition to natural language processing. For this assignment, we will work with a common sentiment analysis task using the IMDB dataset. This set consists of review-label pairs, where the task is to predict whether the text of the given movie review is positive or negative, a binary classification.\n",
    "\n",
    "### RNN Architecture\n",
    "\n",
    "You will be comparing four different recurrent neural network architectures: a standard RNN, a Gated Recurrent Unit (GRU), a standard Long Short-Term Memory (LSTM), and a bidirectional LSTM. \n",
    "\n",
    "Note that a GRU/LSTM cell _is_ an RNN cell, but we will refer to an RNN in the code and questions below as the most basic RNN.\n",
    "\n",
    "### Your Task\n",
    "\n",
    "At the bottom of this notebook file, there are three short answer questions testing your understanding of this neural network architecture. \n",
    "\n",
    "Below each question is a cell with the text “Type Markdown and LaTex.” Double-click the cell and type your response to the question. Save your responses by clicking on the floppy disk icon or choosing File - Save and Checkpoint.\n",
    "\n",
    "After responding to the questions, download your notebook as a `.html` file by choosing File - Download as - html (.html). You will be submitting this `.html` file to your instructor for grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "torch.set_num_threads(4)\n",
    "torch.set_num_interop_threads(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = 'assets_week3'\n",
    "reviewVocabVectors = pickle.load(open(root_dir + '/reviewVocabVectors', 'rb'))\n",
    "trainIterator = pickle.load(open(root_dir + '/trainIterator', 'rb'))\n",
    "testIterator = pickle.load(open(root_dir + '/testIterator', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddingSize = 100\n",
    "hiddenSize = 10\n",
    "dropoutRate = 0.5\n",
    "numEpochs = 5\n",
    "vocabSize = 20002\n",
    "pad = 1\n",
    "unk = 0\n",
    "\n",
    "class MyRNN(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.name = model\n",
    "        self.LSTM = (model == 'LSTM' or model == 'BiLSTM')\n",
    "        self.bidir = (model == 'BiLSTM')\n",
    "        \n",
    "        self.embed = nn.Embedding(vocabSize, embeddingSize, padding_idx = pad)\n",
    "        \n",
    "        if model == 'RNN': \n",
    "            self.rnn = nn.RNN(embeddingSize, hiddenSize)\n",
    "        elif model == 'GRU': \n",
    "            self.rnn = nn.GRU(embeddingSize, hiddenSize)\n",
    "        else: \n",
    "            self.rnn = nn.LSTM(embeddingSize, hiddenSize, bidirectional=self.bidir)\n",
    "\n",
    "        self.dense = nn.Linear(hiddenSize * (2 if self.bidir else 1), 1)\n",
    "        self.dropout = nn.Dropout(dropoutRate)\n",
    "        \n",
    "    def forward(self, text, textLengths):\n",
    "        embedded = self.dropout(self.embed(text))\n",
    "        \n",
    "        packedEmbedded = nn.utils.rnn.pack_padded_sequence(embedded, textLengths)\n",
    "        if self.LSTM: \n",
    "            packedOutput, (hidden, cell) = self.rnn(packedEmbedded)\n",
    "        else: \n",
    "            packedOutput, hidden = self.rnn(packedEmbedded)\n",
    "\n",
    "        output, outputLengths = nn.utils.rnn.pad_packed_sequence(packedOutput)\n",
    "        if self.bidir: \n",
    "            hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n",
    "        else: \n",
    "            hidden = hidden[0]\n",
    "\n",
    "        return self.dense(self.dropout(hidden))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "basicRNN = MyRNN(model='RNN')\n",
    "GRU = MyRNN(model='GRU') # Construct a GRU model, as above\n",
    "LSTM = MyRNN(model='LSTM') # Construct a LSTM model, as above\n",
    "biLSTM = MyRNN(model='BiLSTM') # Construct a BiLSTM model, as above\n",
    "models = [basicRNN, GRU, LSTM, biLSTM]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    if model is None:\n",
    "        continue\n",
    "    model.embed.weight.data.copy_(reviewVocabVectors)\n",
    "    model.embed.weight.data[unk] = torch.zeros(embeddingSize)\n",
    "    model.embed.weight.data[pad] = torch.zeros(embeddingSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "def batchAccuracy(preds, targets):\n",
    "    roundedPreds = (preds >= 0)\n",
    "    return (roundedPreds == targets).sum().item() / len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: RNN, Epoch: 1, Train Loss: 0.7023946511775941\n",
      "Model: RNN, Epoch: 2, Train Loss: 0.6915372308257901\n",
      "Model: RNN, Epoch: 3, Train Loss: 0.6850547656378783\n",
      "Model: RNN, Epoch: 4, Train Loss: 0.6643099472345904\n",
      "Model: RNN, Epoch: 5, Train Loss: 0.6322632445703686\n",
      "\n",
      "Model: GRU, Epoch: 1, Train Loss: 0.7004434326115776\n",
      "Model: GRU, Epoch: 2, Train Loss: 0.6853625318583321\n",
      "Model: GRU, Epoch: 3, Train Loss: 0.6165663341579535\n",
      "Model: GRU, Epoch: 4, Train Loss: 0.4745495242383474\n",
      "Model: GRU, Epoch: 5, Train Loss: 0.38989744939462606\n",
      "\n",
      "Model: LSTM, Epoch: 1, Train Loss: 0.6942260134250612\n",
      "Model: LSTM, Epoch: 2, Train Loss: 0.6677144465544035\n",
      "Model: LSTM, Epoch: 3, Train Loss: 0.5870749384850797\n",
      "Model: LSTM, Epoch: 4, Train Loss: 0.4947256227893293\n",
      "Model: LSTM, Epoch: 5, Train Loss: 0.42434181212006933\n",
      "\n",
      "Model: BiLSTM, Epoch: 1, Train Loss: 0.693617703054872\n",
      "Model: BiLSTM, Epoch: 2, Train Loss: 0.6840410749320789\n",
      "Model: BiLSTM, Epoch: 3, Train Loss: 0.5963251682955896\n",
      "Model: BiLSTM, Epoch: 4, Train Loss: 0.4796233027030135\n",
      "Model: BiLSTM, Epoch: 5, Train Loss: 0.40181239189394297\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "for model in models: \n",
    "    if model is not None:\n",
    "        model.train()\n",
    "\n",
    "for model in models:\n",
    "    if model is None:\n",
    "        continue\n",
    "        \n",
    "    torch.manual_seed(0)\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    for epoch in range(numEpochs):\n",
    "        epochLoss = 0\n",
    "        for batch in trainIterator:\n",
    "            optimizer.zero_grad()\n",
    "            text, textLen = batch[0]\n",
    "            predictions = model(text, textLen).squeeze(1)\n",
    "            loss = criterion(predictions, batch[1])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epochLoss += loss.item()\n",
    "        print(f'Model: {model.name}, Epoch: {epoch + 1}, Train Loss: {epochLoss / len(trainIterator)}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: RNN, Validation Accuracy: 70.63459079283886%\n",
      "Model: GRU, Validation Accuracy: 82.07800511508951%\n",
      "Model: LSTM, Validation Accuracy: 83.81953324808184%\n",
      "Model: BiLSTM, Validation Accuracy: 80.93190537084399%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "\n",
    "for model in models: \n",
    "    if model is not None:\n",
    "        model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    for model in models:\n",
    "        \n",
    "        if model is None:\n",
    "            continue\n",
    "\n",
    "        accuracy = 0.0\n",
    "        for batch in testIterator:\n",
    "            text, textLen = batch[0]\n",
    "            predictions = model(text, textLen).squeeze(1)\n",
    "            loss = criterion(predictions, batch[1])\n",
    "            acc = batchAccuracy(predictions, batch[1])\n",
    "            accuracy += acc\n",
    "        print('Model: {}, Validation Accuracy: {}%'.format(model.name, accuracy / len(testIterator) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6f36ec050380d95b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Homework Questions\n",
    "\n",
    "**To make sure your code produces consistent results, it is advisable to click \"Kernel -> Restart & Run All\" every time you want to run your code.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6a084a6888e27954",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 1: Coding (50 points)\n",
    "\n",
    "First, run the code given above to assess accuracy of the default RNN model. \n",
    "\n",
    "Next, you will need to construct three other model types (GRU, LSTM, BiLSTM) for comparison purposes. Follow the comments in box 5 to initialize the three other model types then rerun the code with all models enabled.\n",
    "\n",
    "Finally, compare the accuracies of all four models (the accuracy of the default RNN should not change from the initial run). Explain your results. In doing so, overview the advantages of the best performing architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-503b047d28162c58",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "**Answer:**\n",
    "\n",
    "The following are the accuracies (rounded off) of the models run for the sentiment analysis task on the IMDb dataset:\n",
    "\n",
    "1. RNN: 70.63%\n",
    "2. GRU: 82.08%\n",
    "3. LSTM: 83.82%\n",
    "4. BiLSTM: 80.93%\n",
    "\n",
    "The vanilla RNN performs the least well unsurprisingly, while the \"gated\" recurrent networks perform much better, with LSTM performing the best amongst all of them.\n",
    "\n",
    "RNNs are typically difficult to parallelize and slow. They also find it difficult to access information from many timesteps back and are susceptible to vanishing and exploding gradients, which explains the poor performance of the vanilla RNN on the task above. On the other hand, \"gated\" networks such as GRU, LSTM, and BiLSTM help in mitigating the problem of vanishing gradients, explaining the superior results. Furthermore, LSTMs are able to control the information flow through many timesteps, allowing the modelling of long-term dependencies unlike vanilla RNNs. GRU provides a comparable performance to that of the LSTM and affords a measure of model parsimony by simplifying the LSTM architecture. The BiLSTM takes into account the contextual information from forward and backward directions by incorporating forward and backward RNNs. While it is more sophisticated than a unidirectional LSTM, it likely overfit to the training data and wasn't able to generalize well with the validation data, making it have a slightly worse validation accuracy than that of the LSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3920676c328b92ee",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 2: LSTM Gates (30 points)\n",
    "\n",
    "LSTMs improve upon the naive RNN architecture by adding a series of gates instead of a simple matrix-vector computation. Name the gates and explain each of their functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-2bb845467aba5dd6",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "**Answer:**\n",
    "\n",
    "The following are the gates of LSTM:\n",
    "\n",
    "1. Forget Gate: It is a weighted non-linear combination of the previous hidden state, the current state, and the bias. It decides whether to keep or forget the information from the previous timestep in the cell given the previous hidden state and the new input data.\n",
    "\n",
    "\n",
    "2. Input Gate: It is also a weighted non-linear combination of the previous hidden state, the current state, and the bias. It decides the extent of cell contents written to the cell state given the previous hidden state and the new input data and is a way to quantify the importance of the new information carried by the input.\n",
    "\n",
    "\n",
    "3. Output Gate: It is a weighted non-linear combination of the previous hidden state, the current state, and the bias as well. It decides what part of the cell contents are generated as an output to the hidden state given the previous hidden state and the new input data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ab00430a80313063",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 3: Applications (20 points)\n",
    "\n",
    "LSTMs are used across many different fields, from music generation to sentiment classification to text generation. Where could they be useful in your life, whether at home, for your family, or in the workplace? Give a specific problem or application for an LSTM model that was not covered in the course slides (**though it can be related to the applications covered in the slides**). Then, with your application in mind, specifically identify the input to your model, the output from your model, and an applicable loss function. \n",
    "\n",
    "(As an optional extension, try implementing your LSTM on your own using the code framework given in this homework!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-51a908d0d4f3f225",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "**Answer:**\n",
    "\n",
    "I often do not have the patience to read large walls of text, especially in the context of informational articles where I just want to learn something quickly or get what I came to that page for without going through the trouble of reading the entire article. A succinct summary of the article would be tremendously helpful for people like me.\n",
    "\n",
    "The problem statement is to summarize the input text (i.e. a large article). Since this is a many-to-many problem, an encoder-decoder architecture, both components of which are LSTMs, would be quite useful in summarizing the input text.\n",
    "\n",
    "The input to my model would be the sequence of words from the large article that I want a summary of. This text would be cleaned up by converting to lower case, removing stopwords, special characters, etc. and would be converted to a context vector by the encoder, which would be the input for the decoder. The decoder would generate the output depending on the information encoded by the encoder. The final output would be the sequence of words that summarize the input text. The loss function applicable in this case would be Categorical Cross-entropy as I would be treating this loosely as a kind of multi-class classification problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
